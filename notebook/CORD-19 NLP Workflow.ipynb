{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORD-19 NLP Workflow\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The CORD-19 dataset [url](https://www.semanticscholar.org/cord19) is an open dataset curated by the Allen Institute for AI intended to facilitate NLP research efforts pertaining to COVID-19. It provides a compilation of relevant peer-reviewed research publications from queries to the following sources: \n",
    "\n",
    "- PubMed's open access corpus\n",
    "- The WHO's open access corpus\n",
    "- bioRxiv and medRxiv preprints\n",
    "\n",
    "The CORD-19 dataset is updated daily, with a .csv of publication metadata provided as well as full text (as available) from articles in JSON format. This dataset is part of a number of NLP research efforts, and is a current Kaggle challenge dataset [url](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "Our goal is to provide a workflow for automating data preparation, processing, and resources for analyses of this dataset. We chose this dataset in part due to its timeliness and relevance, as well as its recurring updates; indeed, it lends itself well to automation. Having a workflow of this sort also foments experiment repeatability and provenance, two crucial elements in accountable AI. \n",
    "\n",
    "With this in mind, we provide below our means for:\n",
    "\n",
    "- Acquiring the dataset\n",
    "- Preparing and processing the data of interest\n",
    "- Generating summary information \n",
    "\n",
    "We chose to focus on the abstract details in the metadata.csv file provided by CORD-19 and the associated full text provided in the pdf_json directory. The former file provides full abstracts from publications, while the latter files provide complete text, both of which we utilize for unsupervised topic modeling via LDA.\n",
    "\n",
    "First we'll import our libraries. We'll use spacy for extracting tokens from the abstracts, nltk for its stopwords corpus as we found it to be more comprehensive than spacy's, gensim to generate a corpus and LDA model, and plotting libraries for visualizing summaries.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first acquire the dataset and extract it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_date='2020-07-16'\n",
    "dataset_filename = 'cord-19_' + dataset_date + '.tar.gz'\n",
    "dataset_url = 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/' + dataset_filename\n",
    "if os.path.exists(dataset_date) != True:\n",
    "    !wget -nc dataset_url\n",
    "    !tar -xzf dataset_filename\n",
    "os.chdir(dataset_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load spacy, and add our own set of words to the NLTK stopwords set, then update spacy's default stop_words set. By doing this, we reduce the potential for generally unique but dataset specific uninteresting and expected words to appear in our topic lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_lg\")\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['sars','covid-19', 'cov-2','=','from', 'subject', 're', 'edu', 'use', 'not', \n",
    "                   'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', \n",
    "                   'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', \n",
    "                   'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', \n",
    "                   'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "nlp.Defaults.stop_words.update(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like we are looking to automate this work on a macro level, we're also fans of the pipeline infrastructure spacy provides, so we define two steps of our NLP pipeline below and add them to our spacy instance. Namely, preprocessing the text provided by splitting into tokens and removing punctuation, stop words, and numerics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc):\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def clean_tokens(doc):\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True and (len(token)<=4)!=True]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(lemmatize,name='lemmatize',after='ner')\n",
    "nlp.add_pipe(clean_tokens, name=\"cleanup\", last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to extract our abstract and publication data from metadata.csv, as well as the full text from the json-ified publication files. We then process each extracted piece of data through our spacy pipeline, then add it to a running list of abstracts and full text. For purposes of resource reduction and speed, the `dataset_limit` variable is provided below. This can be modified to process the entire dataset accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext_list = []\n",
    "abstract_list = []\n",
    "nlp.max_length = 2000000\n",
    "dataset_limit = 100\n",
    "\n",
    "with open('metadata.csv') as f_in:\n",
    "    reader = csv.DictReader(f_in)\n",
    "    for row in tqdm.tqdm_notebook(islice(reader, 0, dataset_limit),total=dataset_limit):\n",
    "        abstract = row['abstract']\n",
    "        all_text = []\n",
    "        if row['pdf_json_files']:\n",
    "            for json_path in row['pdf_json_files'].split('; '):\n",
    "                with open(json_path) as f_json:\n",
    "                    full_text_dict = json.load(f_json)\n",
    "                    for paragraph_dict in full_text_dict['body_text']:\n",
    "                        paragraph_text = paragraph_dict['text']\n",
    "                        section_name = paragraph_dict['section']\n",
    "                        all_text.append(paragraph_text)\n",
    "        nlp_abstract = nlp(abstract)\n",
    "        abstract_list.append(nlp_abstract)\n",
    "        nlp_fulltext = nlp(u' '.join(all_text))\n",
    "        fulltext_list.append(nlp_fulltext)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our document lists in place, we are ready to generated our word dictionary and corpus for abstracts and full text, which we then can use to (finally) create an LDA model of our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(doc_list):\n",
    "    words=corpora.Dictionary(doc_list)\n",
    "    corpus = [words.doc2bow(doc) for doc in doc_list]\n",
    "    return [words,corpus]\n",
    "\n",
    "def generate_lda_model(words,corpus):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=words,\n",
    "                                            num_topics=10, \n",
    "                                            random_state=2,\n",
    "                                            update_every=1,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)  \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_corp = generate_corpus(abstract_list)\n",
    "abstract_words=gen_corp[0]\n",
    "abstract_corpus=gen_corp[1]\n",
    "abstract_lda_model = generate_lda_model(abstract_words,abstract_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_corp = generate_corpus(fulltext_list)\n",
    "fulltext_words=gen_corp[0]\n",
    "fulltext_corpus=gen_corp[1]\n",
    "fulltext_lda_model = generate_lda_model(fulltext_words,fulltext_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now have LDA models generated, we can visualize our analyses. Below we generate word clouds for our topics. Followed by a pyLDAvis gensim specific topic modeling plot showing word strength within topics and distance between topic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWordcloud(lda_model,stop_words):\n",
    "    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "\n",
    "    cloud = WordCloud(stopwords=stop_words,\n",
    "                      background_color='white',\n",
    "                      width=2500,\n",
    "                      height=1800,\n",
    "                      max_words=10,\n",
    "                      colormap='tab10',\n",
    "                      color_func=lambda *args, **kwargs: cols[i],\n",
    "                      prefer_horizontal=1.0)\n",
    "\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        fig.add_subplot(ax)\n",
    "        topic_words = dict(topics[i][1])\n",
    "        cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_wordcloud = plotWordcloud(abstract_lda_model, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext_wordcloud = plotWordcloud(fulltext_lda_model, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_ldavis = pyLDAvis.gensim.prepare(abstract_lda_model, abstract_corpus, dictionary=abstract_lda_model.id2word)\n",
    "abstract_ldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext_ldavis = pyLDAvis.gensim.prepare(fulltext_lda_model, fulltext_corpus, dictionary=fulltext_lda_model.id2word)\n",
    "fulltext_ldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll move relevant pieces of this code to containerized applciations contained within OpenShift, which will then be orchestrated with an Argo workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
