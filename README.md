# CORD-19-NLP

### Overview
The CORD-19 dataset (url)[https://www.semanticscholar.org/cord19] is an open research collection provided by the Allen Institute for AI. Its intent is to facilitate natural language processing research by collecting and annotating scholarly articles pertaining to COVID-19. The dataset is updated daily, with both full text of articles when available and metadata. 

### Workflow motivation
If we consider a use case wherein analysis of the CORD-19 would need to be performed on an as needed basis, the need arising from updated data being made available, we would have at least two operations of importance: 

1. Acquiring the new data
2. Processing the data

This is at minimum and these operations could be broken down into further steps as well. The first operation could be extended to generate a delta between datasets, the second could become preprocessing, feature extraction, evaluation, result generation, and so on. For purposes of this work we focused just on these two steps with the expectation of expanding upon these for specific use cases. 


For this type of application, a repeatable and reliable set of operations is crucial, we don't necessarily need constantly running services for these steps. Instead, we can treat them as on demand operations, which makes an Argo workflow on OpenShift a fitting solution. Indeed, OpenShift provides an enterprise-grade means for creating and operating containerized instances of these operations, whilst Argo provides a simple and powerful way to orchestrate and manage the flow of said containers. 

Our use case then involves wanting autogenerated topic models derived from an instance of the CORD-19 dataset. As such, after doing initial exploratory data analysis and topic modeling on an instance of the CORD-19 dataset, found in the `notebook` directory of this repository, we then extracted acquisition and processing steps from said notebook. These became our workflow application steps, and indeed illustrate the convenience and power of a notebook to containerized application development approach provided by OpenShift. In `data-acqusition` and `data-processing` the containerized applications are provided. 

### Running the workflow
On an OpenShift cluster you are logged into, create a project if you haven't already, for sake of example we'll call it cord-19

`oc new-project cord-19`

Then load the buildconfig and imagestream information for the acquisition and processing applications via the following:

`oc apply -f cord-19-resources.yaml`

Finally, head over to your Argo workflow webui, click Submit New Workflow, and copy paste what's found in `argo-workflow/workflow.yaml`into the textarea that appears. (Note: change `<PROJECT_NAME>` to your project's name, and delete all the sample YAML that Argo provides.

Click Submit and watch the workflow happen.

You can see the topic modeling results by clicking on the log button for the processing node of the workflow.

